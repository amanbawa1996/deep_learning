{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This program is a simple convolution model implemented step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad(X, pad):\n",
    "    \"\"\"\n",
    "    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, \n",
    "    as illustrated in Figure 1.\n",
    "    \n",
    "    Argument:\n",
    "    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
    "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
    "    \n",
    "    Returns:\n",
    "    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line)\n",
    "    X_pad = np.pad(X, ((0,0), (pad,pad), (pad,pad), (0,0)), mode = 'constant', constant_values = (0,0))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape =\n",
      " (4, 3, 3, 2)\n",
      "x_pad.shape =\n",
      " (4, 7, 7, 2)\n",
      "x[1,1] =\n",
      " [[ 0.90085595 -0.68372786]\n",
      " [-0.12289023 -0.93576943]\n",
      " [-0.26788808  0.53035547]]\n",
      "x_pad[1,1] =\n",
      " [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2f0c8ac2e8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Zero Pad testing\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(4, 3, 3, 2)\n",
    "x_pad = zero_pad(x, 2)\n",
    "print (\"x.shape =\\n\", x.shape)\n",
    "print (\"x_pad.shape =\\n\", x_pad.shape)\n",
    "print (\"x[1,1] =\\n\", x[1,1])\n",
    "print (\"x_pad[1,1] =\\n\", x_pad[1,1])\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2)\n",
    "axarr[0].set_title('x')\n",
    "axarr[0].imshow(x[0,:,:,0])\n",
    "axarr[1].set_title('x_pad')\n",
    "axarr[1].imshow(x_pad[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    \"\"\"\n",
    "    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation \n",
    "    of the previous layer.\n",
    "    \n",
    "    Arguments:\n",
    "    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n",
    "    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n",
    "    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
    "    \n",
    "    Returns:\n",
    "    Z -- a scalar value, the result of convolving the sliding window (W, b) on a slice x of the input data\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    # Element-wise product between a_slice_prev and W. Do not add the bias yet.\n",
    "    s = np.multiply(a_slice_prev,W)\n",
    "    # Sum over all entries of the volume s.\n",
    "    Z = np.sum(s)\n",
    "    # Add bias b to Z. Cast b to a float() so that Z results in a scalar value.\n",
    "    Z = Z + float(b)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = -6.999089450680221\n"
     ]
    }
   ],
   "source": [
    "# Testing a single step convolution\n",
    "np.random.seed(1)\n",
    "a_slice_prev = np.random.randn(4, 4, 3)\n",
    "W = np.random.randn(4, 4, 3)\n",
    "b = np.random.randn(1, 1, 1)\n",
    "\n",
    "Z = conv_single_step(a_slice_prev, W, b)\n",
    "print(\"Z =\", Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- output activations of the previous layer, \n",
    "        numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
    "        \n",
    "    Returns:\n",
    "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward() function\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve dimensions from A_prev's shape (≈1 line)  \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = (A_prev.shape[0],A_prev.shape[1], A_prev.shape[2], A_prev.shape[3])\n",
    "    \n",
    "    # Retrieve dimensions from W's shape (≈1 line)\n",
    "    (f, f, n_C_prev, n_C) = (W.shape[0], W.shape[1], W.shape[2], W.shape[3])\n",
    "    \n",
    "    # Retrieve information from \"hparameters\" (≈2 lines)\n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    # Compute the dimensions of the CONV output volume using the formula given above. \n",
    "    # Hint: use int() to apply the 'floor' operation. (≈2 lines)\n",
    "    n_H = int((n_H_prev + 2*pad - f)/stride) + 1\n",
    "    n_W = n_H\n",
    "    \n",
    "    # Initialize the output volume Z with zeros. (≈1 line)\n",
    "    Z = np.zeros((m,n_H, n_W, n_C))\n",
    "    \n",
    "    # Create A_prev_pad by padding A_prev\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    \n",
    "    for i in range(m):               # loop over the batch of training examples\n",
    "        a_prev_pad = A_prev_pad[i,:,:,:]               # Select ith training example's padded activation\n",
    "        for h in range(n_H):           # loop over vertical axis of the output volume\n",
    "            # Find the vertical start and end of the current \"slice\" (≈2 lines)\n",
    "            vert_start = h*stride\n",
    "            vert_end = h*stride + f\n",
    "            \n",
    "            for w in range(n_W):       # loop over horizontal axis of the output volume\n",
    "                # Find the horizontal start and end of the current \"slice\" (≈2 lines)\n",
    "                horiz_start = w*stride\n",
    "                horiz_end = w*stride + f\n",
    "                \n",
    "                for c in range(n_C):   # loop over channels (= #filters) of the output volume\n",
    "                                        \n",
    "                    # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line)\n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:]\n",
    "                    \n",
    "                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈3 line)\n",
    "                    weights = W[:,:,:,c]\n",
    "                    biases = b[:,:,:,c]\n",
    "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, weights, biases)\n",
    "                                        \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Save information in \"cache\" for the backprop\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z's mean =\n",
      " 0.6906325784912735\n",
      "Z[3,2,1] =\n",
      " [-1.28912231  2.27650251  6.61941931  0.95527176  8.25132576  2.31329639\n",
      " 13.00689405  2.34576051]\n",
      "cache_conv[0][1][2][3] =\n",
      " [-1.1191154   1.9560789  -0.3264995  -1.34267579]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10,5,7,4)\n",
    "W = np.random.randn(3,3,4,8)\n",
    "b = np.random.randn(1,1,1,8)\n",
    "hparameters = {\"pad\" : 1,\n",
    "               \"stride\": 2}\n",
    "\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "print(\"Z's mean =\\n\", np.mean(Z))\n",
    "print(\"Z[3,2,1] =\\n\", Z[3,2,1])\n",
    "print(\"cache_conv[0][1][2][3] =\\n\", cache_conv[0][1][2][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the forward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    hparameters -- python dictionary containing \"f\" and \"stride\"\n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters \n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve dimensions from the input shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\"\n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    \n",
    "    # Define the dimensions of the output\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    # Initialize output matrix A\n",
    "    A = np.zeros((m, n_H, n_W, n_C))              \n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    for i in range(m):                         # loop over the training examples\n",
    "        for h in range(n_H):                     # loop on the vertical axis of the output volume\n",
    "            # Find the vertical start and end of the current \"slice\" (≈2 lines)\n",
    "            vert_start = h*stride\n",
    "            vert_end = h*stride+f\n",
    "            \n",
    "            for w in range(n_W):                 # loop on the horizontal axis of the output volume\n",
    "                # Find the vertical start and end of the current \"slice\" (≈2 lines)\n",
    "                horiz_start = w*stride\n",
    "                horiz_end = w*stride+f\n",
    "                \n",
    "                for c in range (n_C):            # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Use the corners to define the current slice on the ith training example of A_prev, channel c. (≈1 line)\n",
    "                    a_prev_slice = A_prev[i,vert_start:vert_end,horiz_start:horiz_end,c]\n",
    "                    \n",
    "                    # Compute the pooling operation on the slice. \n",
    "                    # Use an if statement to differentiate the modes. \n",
    "                    # Use np.max and np.mean.\n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.mean(a_prev_slice)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Store the input and hparameters in \"cache\" for pool_backward()\n",
    "    cache = (A_prev, hparameters)\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(A.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "A.shape = (2, 3, 3, 3)\n",
      "A =\n",
      " [[[[1.74481176 0.90159072 1.65980218]\n",
      "   [1.74481176 1.46210794 1.65980218]\n",
      "   [1.74481176 1.6924546  1.65980218]]\n",
      "\n",
      "  [[1.14472371 0.90159072 2.10025514]\n",
      "   [1.14472371 0.90159072 1.65980218]\n",
      "   [1.14472371 1.6924546  1.65980218]]\n",
      "\n",
      "  [[1.13162939 1.51981682 2.18557541]\n",
      "   [1.13162939 1.51981682 2.18557541]\n",
      "   [1.13162939 1.6924546  2.18557541]]]\n",
      "\n",
      "\n",
      " [[[1.19891788 0.84616065 0.82797464]\n",
      "   [0.69803203 0.84616065 1.2245077 ]\n",
      "   [0.69803203 1.12141771 1.2245077 ]]\n",
      "\n",
      "  [[1.96710175 0.84616065 1.27375593]\n",
      "   [1.96710175 0.84616065 1.23616403]\n",
      "   [1.62765075 1.12141771 1.2245077 ]]\n",
      "\n",
      "  [[1.96710175 0.86888616 1.27375593]\n",
      "   [1.96710175 0.86888616 1.23616403]\n",
      "   [1.62765075 1.12141771 0.79280687]]]]\n",
      "\n",
      "mode = average\n",
      "A.shape = (2, 3, 3, 3)\n",
      "A =\n",
      " [[[[-3.01046719e-02 -3.24021315e-03 -3.36298859e-01]\n",
      "   [ 1.43310483e-01  1.93146751e-01 -4.44905196e-01]\n",
      "   [ 1.28934436e-01  2.22428468e-01  1.25067597e-01]]\n",
      "\n",
      "  [[-3.81801899e-01  1.59993515e-02  1.70562706e-01]\n",
      "   [ 4.73707165e-02  2.59244658e-02  9.20338402e-02]\n",
      "   [ 3.97048605e-02  1.57189094e-01  3.45302489e-01]]\n",
      "\n",
      "  [[-3.82680519e-01  2.32579951e-01  6.25997903e-01]\n",
      "   [-2.47157416e-01 -3.48524998e-04  3.50539717e-01]\n",
      "   [-9.52551510e-02  2.68511000e-01  4.66056368e-01]]]\n",
      "\n",
      "\n",
      " [[[-1.73134159e-01  3.23771981e-01 -3.43175716e-01]\n",
      "   [ 3.80634669e-02  7.26706274e-02 -2.30268958e-01]\n",
      "   [ 2.03009393e-02  1.41414785e-01 -1.23158476e-02]]\n",
      "\n",
      "  [[ 4.44976963e-01 -2.61694592e-03 -3.10403073e-01]\n",
      "   [ 5.08114737e-01 -2.34937338e-01 -2.39611830e-01]\n",
      "   [ 1.18726772e-01  1.72552294e-01 -2.21121966e-01]]\n",
      "\n",
      "  [[ 4.29449255e-01  8.44699612e-02 -2.72909051e-01]\n",
      "   [ 6.76351685e-01 -1.20138225e-01 -2.44076712e-01]\n",
      "   [ 1.50774518e-01  2.89111751e-01  1.23238536e-03]]]]\n"
     ]
    }
   ],
   "source": [
    "# Case 1: stride of 1\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 5, 5, 3)\n",
    "hparameters = {\"stride\" : 1, \"f\": 3}\n",
    "\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "print(\"mode = max\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A =\\n\", A)\n",
    "print()\n",
    "A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A =\\n\", A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "A.shape = (2, 2, 2, 3)\n",
      "A =\n",
      " [[[[1.74481176 0.90159072 1.65980218]\n",
      "   [1.74481176 1.6924546  1.65980218]]\n",
      "\n",
      "  [[1.13162939 1.51981682 2.18557541]\n",
      "   [1.13162939 1.6924546  2.18557541]]]\n",
      "\n",
      "\n",
      " [[[1.19891788 0.84616065 0.82797464]\n",
      "   [0.69803203 1.12141771 1.2245077 ]]\n",
      "\n",
      "  [[1.96710175 0.86888616 1.27375593]\n",
      "   [1.62765075 1.12141771 0.79280687]]]]\n",
      "\n",
      "mode = average\n",
      "A.shape = (2, 2, 2, 3)\n",
      "A =\n",
      " [[[[-0.03010467 -0.00324021 -0.33629886]\n",
      "   [ 0.12893444  0.22242847  0.1250676 ]]\n",
      "\n",
      "  [[-0.38268052  0.23257995  0.6259979 ]\n",
      "   [-0.09525515  0.268511    0.46605637]]]\n",
      "\n",
      "\n",
      " [[[-0.17313416  0.32377198 -0.34317572]\n",
      "   [ 0.02030094  0.14141479 -0.01231585]]\n",
      "\n",
      "  [[ 0.42944926  0.08446996 -0.27290905]\n",
      "   [ 0.15077452  0.28911175  0.00123239]]]]\n"
     ]
    }
   ],
   "source": [
    "# Case 2: stride of 2\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 5, 5, 3)\n",
    "hparameters = {\"stride\" : 2, \"f\": 3}\n",
    "\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "print(\"mode = max\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A =\\n\", A)\n",
    "print()\n",
    "\n",
    "A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A =\\n\", A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "          numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "          numpy array of shape (1, 1, 1, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve information from \"cache\"\n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = (A_prev.shape[0], A_prev.shape[1], A_prev.shape[2], A_prev.shape[3])\n",
    "    \n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, n_C_prev, n_C) = (W.shape[0], W.shape[1], W.shape[2], W.shape[3])\n",
    "    \n",
    "    # Retrieve information from \"hparameters\"\n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    # Retrieve dimensions from dZ's shape\n",
    "    (m, n_H, n_W, n_C) = (dZ.shape[0], dZ.shape[1], dZ.shape[2], dZ.shape[3])\n",
    "    \n",
    "    # Initialize dA_prev, dW, db with the correct shapes\n",
    "    dA_prev = np.zeros((m,n_H_prev, n_W_prev, n_C_prev))                           \n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1,1,1,n_C))\n",
    "\n",
    "    # Pad A_prev and dA_prev\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        \n",
    "        # select ith training example from A_prev_pad and dA_prev_pad\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        da_prev_pad = dA_prev_pad[i]\n",
    "        \n",
    "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):           # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\"\n",
    "                    vert_start = h*stride \n",
    "                    vert_end = h*stride+f\n",
    "                    horiz_start = w*stride\n",
    "                    horiz_end = w*stride+f\n",
    "                    \n",
    "                    # Use the corners to define the slice from a_prev_pad\n",
    "                    a_slice = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:]\n",
    "\n",
    "                    # Update gradients for the window and the filter's parameters using the code formulas given above\n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c]*dZ[i,h,w,c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i,h,w,c]\n",
    "                    \n",
    "        # Set the ith training example's dA_prev to the unpadded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])\n",
    "        dA_prev[i, :, :, :] = da_prev_pad[pad:da_prev_pad.shape[0] - pad, pad: da_prev_pad.shape[1]-pad,:]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_mean = 1.4524377775388075\n",
      "dW_mean = 1.7269914583139097\n",
      "db_mean = 7.839232564616838\n"
     ]
    }
   ],
   "source": [
    "# We'll run conv_forward to initialize the 'Z' and 'cache_conv\",\n",
    "# which we'll use to test the conv_backward function\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10,4,4,3)\n",
    "W = np.random.randn(2,2,3,8)\n",
    "b = np.random.randn(1,1,1,8)\n",
    "hparameters = {\"pad\" : 2,\n",
    "               \"stride\": 2}\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "\n",
    "# Test conv_backward\n",
    "dA, dW, db = conv_backward(Z, cache_conv)\n",
    "print(\"dA_mean =\", np.mean(dA))\n",
    "print(\"dW_mean =\", np.mean(dW))\n",
    "print(\"db_mean =\", np.mean(db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "    \"\"\"\n",
    "    Creates a mask from an input matrix x, to identify the max entry of x.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- Array of shape (f, f)\n",
    "    \n",
    "    Returns:\n",
    "    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈1 line)\n",
    "    mask = (x == np.max(x))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [[ 1.62434536 -0.61175641 -0.52817175]\n",
      " [-1.07296862  0.86540763 -2.3015387 ]]\n",
      "mask =  [[ True False False]\n",
      " [False False False]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(2,3)\n",
    "mask = create_mask_from_window(x)\n",
    "print('x = ', x)\n",
    "print(\"mask = \", mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_value(dz, shape):\n",
    "    \"\"\"\n",
    "    Distributes the input value in the matrix of dimension shape\n",
    "    \n",
    "    Arguments:\n",
    "    dz -- input scalar\n",
    "    shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz\n",
    "    \n",
    "    Returns:\n",
    "    a -- Array of size (n_H, n_W) for which we distributed the value of dz\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve dimensions from shape (≈1 line)\n",
    "    (n_H, n_W) = shape\n",
    "    \n",
    "    # Compute the value to distribute on the matrix (≈1 line)\n",
    "    average = dz/(n_H+n_W)\n",
    "    \n",
    "    # Create a matrix where every entry is the \"average\" value (≈1 line)\n",
    "    a = np.ones((n_H,n_W))*average\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distributed value = [[0.5 0.5]\n",
      " [0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "a = distribute_value(2, (2,2))\n",
    "print('distributed value =', a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_backward(dA, cache, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the backward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n",
    "    cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters \n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Retrieve information from cache (≈1 line)\n",
    "    (A_prev, hparameters) = cache\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\" (≈2 lines)\n",
    "    stride = hparameters[\"stride\"]\n",
    "    f = hparameters[\"f\"]\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape and dA's shape (≈2 lines)\n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape\n",
    "    \n",
    "    # Initialize dA_prev with zeros (≈1 line)\n",
    "    dA_prev = np.zeros(A_prev.shape)\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        \n",
    "        # select training example from A_prev (≈1 line)\n",
    "        a_prev = A_prev[i]\n",
    "        \n",
    "        for h in range(n_H):                   # loop on the vertical axis\n",
    "            for w in range(n_W):               # loop on the horizontal axis\n",
    "                for c in range(n_C):           # loop over the channels (depth)\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = h*stride\n",
    "                    vert_end = h*stride + f\n",
    "                    horiz_start = w*stride\n",
    "                    horiz_end = w*stride + f\n",
    "                    \n",
    "                    # Compute the backward propagation in both modes.\n",
    "                    if mode == \"max\":\n",
    "                        \n",
    "                        # Use the corners and \"c\" to define the current slice from a_prev (≈1 line)\n",
    "                        a_prev_slice = a_prev[vert_start:vert_end,horiz_start:horiz_end,c]\n",
    "                        # Create the mask from a_prev_slice (≈1 line)\n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "                        # Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA) (≈1 line)\n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += mask*dA[i,h,w, c]\n",
    "                        \n",
    "                    elif mode == \"average\":\n",
    "                        \n",
    "                        # Get the value a from dA (≈1 line)\n",
    "                        da = dA[i,h,w,c]\n",
    "                        # Define the shape of the filter as fxf (≈1 line)\n",
    "                        shape = (f,f)\n",
    "                        # Distribute it to get the correct slice of dA_prev. i.e. Add the distributed value of da. (≈1 line)\n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += distribute_value(da,shape)\n",
    "                        \n",
    "    ### END CODE ###\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev[1,1] =  [[ 0.          0.        ]\n",
      " [ 5.05844394 -1.68282702]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "mode = average\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev[1,1] =  [[ 0.08485462  0.2787552 ]\n",
      " [ 1.26461098 -0.25749373]\n",
      " [ 1.17975636 -0.53624893]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(5, 5, 3, 2)\n",
    "hparameters = {\"stride\" : 1, \"f\": 2}\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "dA = np.random.randn(5, 4, 2, 2)\n",
    "\n",
    "dA_prev = pool_backward(dA, cache, mode = \"max\")\n",
    "print(\"mode = max\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1])  \n",
    "print()\n",
    "dA_prev = pool_backward(dA, cache, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
